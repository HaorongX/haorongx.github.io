<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 7.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.2/css/all.min.css" integrity="sha256-XOqroi11tY4EFQMR9ZYwZWKj5ZXiftSx36RRuC3anlA=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"haorongx.github.io","root":"/","images":"/images","scheme":"Mist","darkmode":false,"version":"8.20.0","exturl":false,"sidebar":{"position":"left","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="Glossary  Supervised Learning \(^{[1, 2]}\): An machine learning apporach in which labeled data is used. Labeled data tells the model what a correct output should be (supervised), which enables the mo">
<meta property="og:type" content="article">
<meta property="og:title" content="Intro to Decision Tree">
<meta property="og:url" content="https://haorongx.github.io/2025/01/04/Intro-to-Decision-Tree/index.html">
<meta property="og:site_name" content="Haorong&#39;s Blog">
<meta property="og:description" content="Glossary  Supervised Learning \(^{[1, 2]}\): An machine learning apporach in which labeled data is used. Labeled data tells the model what a correct output should be (supervised), which enables the mo">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://haorongx.github.io/2025/01/04/Intro-to-Decision-Tree/images/decisiontree1.png">
<meta property="og:image" content="https://haorongx.github.io/2025/01/04/Intro-to-Decision-Tree/images/decisiontree_overfit.png">
<meta property="article:published_time" content="2025-01-04T07:36:33.000Z">
<meta property="article:modified_time" content="2025-01-30T15:03:35.000Z">
<meta property="article:author" content="Haorong Xu">
<meta property="article:tag" content="AI">
<meta property="article:tag" content="Decision Tree">
<meta property="article:tag" content="Random Forest">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://haorongx.github.io/2025/01/04/Intro-to-Decision-Tree/images/decisiontree1.png">


<link rel="canonical" href="https://haorongx.github.io/2025/01/04/Intro-to-Decision-Tree/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"en","comments":true,"permalink":"https://haorongx.github.io/2025/01/04/Intro-to-Decision-Tree/","path":"2025/01/04/Intro-to-Decision-Tree/","title":"Intro to Decision Tree"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>Intro to Decision Tree | Haorong's Blog</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --></head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Haorong's Blog</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">To remove all barrieres in the way of computer science.</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="Search" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#glossary"><span class="nav-number">1.</span> <span class="nav-text">Glossary</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#intro"><span class="nav-number">2.</span> <span class="nav-text">Intro</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#decision-tree-learning-algorithms"><span class="nav-number">3.</span> <span class="nav-text">Decision Tree Learning
Algorithms</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#iterative-dichotomiser-3"><span class="nav-number">4.</span> <span class="nav-text">Iterative Dichotomiser 3</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#information-gain-entropy"><span class="nav-number">4.1.</span> <span class="nav-text">Information Gain &amp; Entropy</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#entropy"><span class="nav-number">4.1.1.</span> <span class="nav-text">Entropy</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#information-gain"><span class="nav-number">4.1.2.</span> <span class="nav-text">Information Gain</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#example"><span class="nav-number">4.1.3.</span> <span class="nav-text">Example</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#implementation"><span class="nav-number">4.2.</span> <span class="nav-text">Implementation</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#references"><span class="nav-number">5.</span> <span class="nav-text">References</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Haorong Xu"
      src="https://avatars.githubusercontent.com/u/103825064?v=4">
  <p class="site-author-name" itemprop="name">Haorong Xu</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">11</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">8</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">17</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="mailto:db_haorong@outlook.com" title="E-Mail → mailto:db_haorong@outlook.com" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="https://haorongx.github.io/2025/01/04/Intro-to-Decision-Tree/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="https://avatars.githubusercontent.com/u/103825064?v=4">
      <meta itemprop="name" content="Haorong Xu">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Haorong's Blog">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="Intro to Decision Tree | Haorong's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Intro to Decision Tree
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2025-01-04 15:36:33" itemprop="dateCreated datePublished" datetime="2025-01-04T15:36:33+08:00">2025-01-04</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2025-01-30 23:03:35" itemprop="dateModified" datetime="2025-01-30T23:03:35+08:00">2025-01-30</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/AI/" itemprop="url" rel="index"><span itemprop="name">AI</span></a>
        </span>
    </span>

  
    <span class="post-meta-item" title="Views" id="busuanzi_container_page_pv">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">Views: </span>
      <span id="busuanzi_value_page_pv"></span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><h2 id="glossary">Glossary</h2>
<ul>
<li>Supervised Learning <span class="math inline">\(^{[1, 2]}\)</span>:
An machine learning apporach in which <strong>labeled</strong> data is
used. Labeled data tells the model what a correct output should be
(supervised), which enables the model to minimize the error and produce
desired output.</li>
<li>Unsupervised Learning: On the other hand, unsupervised learning uses
<strong>unlabeled</strong> data (i.e. No additional guidance, therefore
unsupervised). The model then tries to find the similarities or hidden
patterns in input data.</li>
<li>Classification <span class="math inline">\(^{[3]}\)</span>:
Classification is a typical task of supervised learning, which is
predicting a <strong>categorical value</strong>.</li>
<li>Regression: As another task of supervised learning, regression aims
to predict a <strong>continuous numerical value</strong>.</li>
</ul>
<h2 id="intro">Intro</h2>
<p>Decision tree is an supervised learning algorithm that can carry out
both classification and regression. A decision tree consists of three
types of nodes (just like any other trees), a root node, internal nodes,
and leaf nodes. An internal node or root node represents a choice
avaliable at the current point of the tree, leading to several mutually
exclusive results, which are its children nodes <span
class="math inline">\(^{[4]}\)</span>. Each leaf node represents a final
outcome.</p>
<figure>
<img src="images/decisiontree1.png"
alt="A decision tree to classify iPhone models" />
<figcaption aria-hidden="true">A decision tree to classify iPhone
models</figcaption>
</figure>
<p>It is worth highlighting that a decision tree can be a binary tree
(in many cases), but not necessarily.</p>
<p>Decision tree has several advantages, including ease to interpret,
and low complexity (logarithm). However, there is a major drawback,
overfitting. Overfitting is a common problem, which refers to that a
model adapts too well to its training data but fails to make accurate
prediction from any other data <span
class="math inline">\(^{[5]}\)</span>.</p>
<figure>
<img src="images/decisiontree_overfit.png" alt="Overfitting" />
<figcaption aria-hidden="true">Overfitting</figcaption>
</figure>
<p>As shown in the diagram above, each blue point represents a
individual data in the trainning set, while the green line illustrates a
good fit, the red curve is an example of overfitting. The red curve
shows a polynomial function that fits perfectly to the training data,
since it passes through every single blue point. However, it clearly
fails to show the overall trend of the data, making it completely
worthless. In contrast, the green line represents the data overall well,
although some points are not on the line.</p>
<p>For a decision tree, it can easily become overfit when it grows too
large or complicated. Therefore, a widely used strategy to avoid
overfitting is stopping the tree splitting when it is necessary. This
strategy can be implemented by setting maximum depth, minimum records
number per leaf node, etc. There is another technique called pruning, in
which nodes with less information are removed, but we shall not discuss
this technique in this article.</p>
<h2 id="decision-tree-learning-algorithms">Decision Tree Learning
Algorithms</h2>
<p>There are several algorithms for constructing decision trees,
including ID3, C4.5, CHAID, CRAT, and QUEST. In this article, we explore
the well-know algorithm, ID3, in detail.</p>
<h2 id="iterative-dichotomiser-3">Iterative Dichotomiser 3</h2>
<p>The word 'Dichotomiser' refers to dividing things into two completely
opposing ideas <span class="math inline">\(^{[6]}\)</span>. As its name
suggests, ID3 typically creates a binary tree. However, it can also
creates multi-way splits. ID3 builds the tree recursively in the
following way <span class="math inline">\(^{[7]}\)</span>:</p>
<ol type="1">
<li>A <em>best</em> attribute is selected (based on <strong>information
gain</strong>, or <strong>entropy</strong>, described in later section).
This is done by a greedy approach, which means the attribute with least
entropy or most information gain is selected.</li>
<li>The dataset is then split based on its distinct value.</li>
<li>For each subset, this process is repeated (recursion).</li>
</ol>
<p>It is worth noting that ID3 does not include a pruning part, although
some of its variants (e.g. C4.5) does. Instead, ID3 avoids overfitting
by setting stopping criteria.</p>
<h3 id="information-gain-entropy">Information Gain &amp; Entropy</h3>
<h4 id="entropy">Entropy</h4>
<p>Entropy (denoted as <span class="math inline">\(H\)</span>) measures
how uncertain or disordered the dataset is. Uncertainty in this case
stands for how <strong>unpredictable</strong> the dataset is. For
example, the probability of an asteroid colliding with the earth today
is 0.00..1% so it's <strong>easy to predict</strong> that this event is
not going to take place today which means low unpredictability (low
entropy). In contrast, the probability of obtaining a head when tossing
a unbiased coin is 50%, so there is a high uncertainty (high
entropy).</p>
<p>Entropy can be caculated as</p>
<p><span class="math display">\[H(S) = - \sum^{n}_{i = 1} p_i \cdot
log_2(p_i)\]</span></p>
<p>where <span class="math inline">\(S\)</span> stands for the dataset
<span class="math inline">\({\{c_1, c_2, c_3, ...\}}\)</span> and <span
class="math inline">\(p\)</span> represents the
<strong>proportion</strong> of the number of elements in class <span
class="math inline">\(c_i\)</span> to the number of elements in dataset
<span class="math inline">\(S\)</span> (i.e. Probability of this
particular outcome).</p>
<p>Entropy measures the weighted contribution of a certain class to the
overall uncertainty of the dataset. Obviously, high proportion (common
event) indicates low contribution to uncertainty, which is indicated in
the formula by <span class="math inline">\(log_2(p_i)\)</span>. Given
that <span class="math inline">\(0 &lt; p_i \leq 1\)</span>, <span
class="math inline">\(log_2(p_i)\)</span> declines as <span
class="math inline">\(p_i\)</span> rises, standing for lower
contribution to entropy (vice versa). This result is then weighted by
multiplying it by its proportion. Finally, since <span
class="math inline">\(log_2(p_i) &lt; 0\)</span> in this case, a
negative sign is added.</p>
<h4 id="information-gain">Information Gain</h4>
<p>Based on entropy, information gain is the expected reduction in an
entropy because of sorting on an attribute <span
class="math inline">\(^{[8]}\)</span>, defined as</p>
<p><span class="math display">\[IG = H(S)-\sum^k_{j = 1}
\frac{|S_j|}{|S|} \cdot H(S_j)\]</span></p>
<p>Where <span class="math inline">\(S_j\)</span> stands for subsets of
<span class="math inline">\(S\)</span> after splitting on a feature.</p>
<p>Notice that this formula calculates the difference between the
original entropy of the dataset and the summation of the entropy after
the split, which can be summarized as <strong>the reduction in entropy
after a split</strong>.</p>
<h4 id="example">Example</h4>
<p>The following table presents a dataset, consisting of students' study
hours, with each record being labelled as either 'Pass' or 'Fail'.</p>
<table>
<thead>
<tr class="header">
<th>Study Hours</th>
<th>Result</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>Fail</td>
</tr>
<tr class="even">
<td>2</td>
<td>Pass</td>
</tr>
<tr class="odd">
<td>3</td>
<td>Fail</td>
</tr>
<tr class="even">
<td>4</td>
<td>Fail</td>
</tr>
<tr class="odd">
<td>5</td>
<td>Pass</td>
</tr>
<tr class="even">
<td>6</td>
<td>Fail</td>
</tr>
<tr class="odd">
<td>7</td>
<td>Pass</td>
</tr>
<tr class="even">
<td>8</td>
<td>Pass</td>
</tr>
<tr class="odd">
<td>9</td>
<td>Pass</td>
</tr>
<tr class="even">
<td>10</td>
<td>Pass</td>
</tr>
</tbody>
</table>
<p>Total: 4 Fails, 6 Passes</p>
<p>The original entropy before the split is</p>
<p><span class="math display">\[H(S) = -(0.4 \times log_2(0.4) + 0.6
\times log_2(0.6)) \approx 0.971\]</span></p>
<p>After a split on study hour:</p>
<table>
<thead>
<tr class="header">
<th>Study Hours</th>
<th>Num. of Passes</th>
<th>Num. of Fails</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(\leq 5\)</span> hrs</td>
<td>2</td>
<td>3</td>
</tr>
<tr class="even">
<td>$ &gt; 5 $ hrs</td>
<td>4</td>
<td>1</td>
</tr>
</tbody>
</table>
<p>After the split, the aggregated entropy is</p>
<p><span class="math display">\[ H(S) = -[ 0.5*(0.4 * log_2(0.4) + 0.6 *
log_2(0.6)) + 0.5*(0.8 * log_2(0.8) + 0.2 * log_2(0.2)) ] \approx
0.846\]</span></p>
<p>Therefore, <span class="math inline">\(IG = 0.971 - 0.846 =
0.125\)</span></p>
<h3 id="implementation">Implementation</h3>
<p>As described earlier, ID3 constructs a decision tree by splitting on
a feature with greatest information gain recursively.</p>
<p>In this article, we presents a decision tree that accepts only one
numerical feature and a binary (true-or-false) label to illustrate the
basic mechanism of decision tree.</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> math <span class="keyword">import</span> log2</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">node</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, val = <span class="literal">None</span>, res = <span class="literal">None</span>, ltree = <span class="literal">None</span>, rtree = <span class="literal">None</span></span>):</span><br><span class="line">        <span class="variable language_">self</span>.val = val</span><br><span class="line">        <span class="variable language_">self</span>.res = res</span><br><span class="line">        <span class="variable language_">self</span>.ltree = ltree <span class="comment"># Left sub-tree indicates false</span></span><br><span class="line">        <span class="variable language_">self</span>.rtree = rtree <span class="comment"># Right side indicates true</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">entropy</span>(<span class="params">data</span>):</span><br><span class="line">    cnt_true = <span class="number">0</span></span><br><span class="line">    cnt_false = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> data:</span><br><span class="line">        <span class="keyword">if</span> i == <span class="number">0</span>:</span><br><span class="line">            cnt_false += <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            cnt_true += <span class="number">1</span></span><br><span class="line">    <span class="keyword">if</span> cnt_true == <span class="number">0</span> <span class="keyword">or</span> cnt_false == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">    total = <span class="built_in">len</span>(data)</span><br><span class="line">    <span class="keyword">return</span> -((cnt_true / total) * log2(cnt_true / total) + (cnt_false / total) * log2(cnt_false / total))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">build</span>(<span class="params">feature, label</span>): <span class="comment"># We assume feature is sorted in ascending order for convenience</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(label) == <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> node(res = label[<span class="number">0</span>])</span><br><span class="line">    max_ig = <span class="number">0</span></span><br><span class="line">    best_threshold = -<span class="number">1</span></span><br><span class="line">    <span class="built_in">print</span>(feature, label)</span><br><span class="line">    original_entropy = entropy(label)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(feature)):</span><br><span class="line">        l_feature = feature[:i]</span><br><span class="line">        l_label = label[:i]</span><br><span class="line">        r_feature = feature[i:]</span><br><span class="line">        r_label = label[i:]</span><br><span class="line">        p = i / <span class="built_in">len</span>(feature)</span><br><span class="line">        new_entropy = p * entropy(l_label) + (<span class="number">1</span> - p) * entropy(r_label)</span><br><span class="line">        new_ig = original_entropy - new_entropy</span><br><span class="line">        <span class="keyword">if</span> new_ig &gt; max_ig:</span><br><span class="line">            max_ig = new_ig</span><br><span class="line">            best_threshold = i</span><br><span class="line">    <span class="keyword">if</span> max_ig == <span class="number">0</span>: <span class="comment"># This node is &#x27;pure&#x27;!</span></span><br><span class="line">        <span class="keyword">return</span> node(res = label[<span class="number">0</span>])</span><br><span class="line">    <span class="keyword">return</span> node(val = feature[best_threshold], ltree = build(feature[:best_threshold], label[:best_threshold]), rtree = build(feature[best_threshold:], label[best_threshold:]))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">predict</span>(<span class="params">val, cur</span>):</span><br><span class="line">    <span class="keyword">if</span> cur.res != <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">return</span> cur.res</span><br><span class="line">    <span class="keyword">if</span> val &lt; cur.val:</span><br><span class="line">        <span class="keyword">return</span> predict(val, cur.ltree)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> predict(val, cur.rtree)</span><br></pre></td></tr></table></figure>
<p>Using the example in the article, we can train our model by</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">study_hours = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>]</span><br><span class="line">result = [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>] <span class="comment"># 0 = Fail, 1 = Pass</span></span><br><span class="line">decision_tree = build(study_hours, result)</span><br></pre></td></tr></table></figure>
<p>Now we can use our decision tree to predict the exam result with
study hour given:</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="number">15</span>):</span><br><span class="line">    <span class="built_in">print</span>(i, predict(i, decision_tree))</span><br></pre></td></tr></table></figure>
<p>The results turned out to be</p>
<table>
<thead>
<tr class="header">
<th>Study Hours</th>
<th>Result</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>0</td>
</tr>
<tr class="even">
<td>1</td>
<td>0</td>
</tr>
<tr class="odd">
<td>2</td>
<td>1</td>
</tr>
<tr class="even">
<td>3</td>
<td>0</td>
</tr>
<tr class="odd">
<td>4</td>
<td>0</td>
</tr>
<tr class="even">
<td>5</td>
<td>1</td>
</tr>
<tr class="odd">
<td>6</td>
<td>0</td>
</tr>
<tr class="even">
<td>7</td>
<td>1</td>
</tr>
<tr class="odd">
<td>8</td>
<td>1</td>
</tr>
<tr class="even">
<td>9</td>
<td>1</td>
</tr>
<tr class="odd">
<td>10</td>
<td>1</td>
</tr>
<tr class="even">
<td>11</td>
<td>1</td>
</tr>
<tr class="odd">
<td>12</td>
<td>1</td>
</tr>
<tr class="even">
<td>13</td>
<td>1</td>
</tr>
<tr class="odd">
<td>14</td>
<td>1</td>
</tr>
</tbody>
</table>
<h2 id="references">References</h2>
<ul>
<li>[1] <em>Supervised versus unsupervised learning: What's the
difference?</em>, IBM, <a
target="_blank" rel="noopener" href="https://www.ibm.com/think/topics/supervised-vs-unsupervised-learning"
class="uri">https://www.ibm.com/think/topics/supervised-vs-unsupervised-learning</a></li>
<li>[2] <em>Supervised vs. unsupervised learning: What's the
difference?</em>, Google Cloud, <a
target="_blank" rel="noopener" href="https://cloud.google.com/discover/supervised-vs-unsupervised-learning"
class="uri">https://cloud.google.com/discover/supervised-vs-unsupervised-learning</a></li>
<li>[3] <em>What is the difference between classification and regression
in supervised machine learning?</em>, Scribbr, <a
target="_blank" rel="noopener" href="https://www.scribbr.com/frequently-asked-questions/what-is-the-difference-between-classification-and-regression-in-supervised-machine-learning"
class="uri">https://www.scribbr.com/frequently-asked-questions/what-is-the-difference-between-classification-and-regression-in-supervised-machine-learning</a></li>
<li>[4] Song, Yanyan and Ying Lu. <em>“Decision tree methods:
applications for classification and prediction.”</em> Shanghai Archives
of Psychiatry 27 (2015): 130 - 135</li>
<li>[5] <em>What is overfitting</em>, IBM, <a
target="_blank" rel="noopener" href="https://www.ibm.com/think/topics/overfitting"
class="uri">https://www.ibm.com/think/topics/overfitting</a></li>
<li>[6] <em>DICHOTOMIST</em>, Cambridge Dictionary</li>
<li>[7] <em>Iterative Dichotomiser 3 (ID3) Algorithm From Scratch</em>,
Geeks for Geeks, <a
target="_blank" rel="noopener" href="https://www.geeksforgeeks.org/iterative-dichotomiser-3-id3-algorithm-from-scratch/"
class="uri">https://www.geeksforgeeks.org/iterative-dichotomiser-3-id3-algorithm-from-scratch/</a></li>
<li>[8] <em>Handbook of Statistics</em>, Gangadhar Shobha, Shanta
Rangaswamy, 2018, Elsevier, Volume 38, pp. 197 - 228, <a
target="_blank" rel="noopener" href="https://doi.org/10.1016/bs.host.2018.07.004"
class="uri">https://doi.org/10.1016/bs.host.2018.07.004</a></li>
</ul>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/AI/" rel="tag"># AI</a>
              <a href="/tags/Decision-Tree/" rel="tag"># Decision Tree</a>
              <a href="/tags/Random-Forest/" rel="tag"># Random Forest</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2024/11/03/Arthimetic-Instructions-in-RV32I/" rel="prev" title="Basic Arithimetic Instructions in RV32I">
                  <i class="fa fa-angle-left"></i> Basic Arithimetic Instructions in RV32I
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2025/05/17/Auto-Categorizing-Past-Papers/" rel="next" title="Generating a Question Bank from Past Papers Automatically">
                  Generating a Question Bank from Past Papers Automatically <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2025</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">Haorong Xu</span>
  </div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="Total Visitors">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="Total Views">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>

  <a href="https://github.com/HaorongX" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/sidebar.js"></script><script src="/js/next-boot.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>







  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>
